---
title: "Lab 7"
author: "Insert Name"
date: "Math 241, Week 9"
output:
  pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
# Do not modify this chunk.
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)


```

```{r}
#install.packages("tidytext")
#install.packages("wordcloud")
#install.packages("tm")

# Put all necessary libraries here
library(tidyverse)
library(tidytext)
library(wordcloud)
library(RColorBrewer)

# Ensure the textdata package is installed
if (!requireNamespace("textdata", quietly = TRUE)) {
  install.packages("textdata")
}
# Load the textdata package
library(textdata)

# Before knitting your document one last time, you will have to download the AFINN lexicon explicitly
lexicon_afinn()
afinn <- lexicon_afinn()
lexicon_nrc()
```



## Due: Friday, March 29th at 5:30pm

## Goals of this lab

1. Practice matching patterns with regular expressions.
1. Practice manipulating strings with `stringr`.
1. Practice tokenizing text with `tidytext`.
1. Practice looking at word frequencies.
1. Practice conducting sentiment analysis.


### Problem 1: What's in a Name?  (You'd Be Surprised!)
  
1. Load the `babynames` dataset, which contains yearly information on the frequency of baby names by sex and is provided by the US Social Security Administration.  It includes all names with at least 5 uses per year per sex. In this problem, we are going to practice pattern matching!

```{r}
library(babynames)
data("babynames")
#?babynames
```

a. For 2000, find the ten most popular female baby names that start with the letter Z.

```{r, eval = FALSE}

# Convert names to lowercase
babynames$name <- tolower(babynames$name)

# Filter for names starting with 'z' for females in the year 2000
z_names <- babynames %>%
  filter(year == 2000) %>%
  filter(sex == "F") %>%
  filter(startsWith(name, "z")) %>%
  top_n(10)

z_names

```




b. For 2000, find the ten most popular female baby names that contain the letter z.  

```{r}

# Filter for names containing 'z'
z_containing_names <- babynames %>%
  filter(year == 2000) %>%
  filter(sex == "F") %>%
  filter(grepl("z", name)) %>% 
  top_n(10)

z_containing_names


```


c. For 2000, find the ten most popular female baby names that end in the letter z. 
```{r}


# Filter for female names ending with 'z' for the year 2000 and select the top 10
z_end_names <- babynames %>%
  filter(year == 2000) %>%
  filter(sex == "F") %>%
  filter(endsWith(name, "z")) %>%
  top_n(10)

z_end_names
  
```

d. Between your three tables in 1.a - 1.c, do any of the names show up on more than one list?  If so, which ones? (Yes, I know you could do this visually but use some joins!)

```{r}

#im gonna do this as one big code chunk


# Filter for top 10 names starting with 'z' for females in the year 2000
z_start_names <- babynames %>%
  filter(year == 2000) %>%
  filter(sex == "F") %>%
  filter(startsWith(name, "z")) %>%
  top_n(10)

# Filter for top 10 names ending with 'z' for females in the year 2000
z_end_names <- babynames %>%
  filter(year == 2000) %>%
  filter(sex == "F") %>%
  filter(endsWith(name, "z")) %>%
  top_n(10)

# Filter for top 10 names containing 'z' for females in the year 2000
z_containing_names <- babynames %>%
  filter(year == 2000) %>%
  filter(sex == "F") %>%
  filter(grepl("z", name)) %>%
  top_n(10)

# Join all three top ten lists
z_names <- inner_join(z_start_names, z_end_names, by = "name") %>%
  inner_join(z_containing_names, by = "name")

z_names
# I guess theres no overlap?? 

```




e.  Verify that none of the baby names contain a numeric (0-9) in them.
```{r}


# Check for names containing numeric characters
names_with_numeric <- babynames[grepl("[0-9]", babynames$name), ]

# View the first few rows of the filtered dataset
head(names_with_numeric)


#nope, none

```


f. While none of the names contain 0-9, that doesn't mean they don't contain "one", "two", ..., or "nine".  Create a table that provides the number of times a baby's name contained the word "zero", the word "one", ... the word "nine". 

```{r}

# Check for names containing numbers
names_with_numeric <- babynames[grepl("one|two|three|four|five|six|seven|eight|nine", babynames$name), ]

# View the first few rows of the filtered dataset
head(names_with_numeric)



```


Notes: 

* I recommend first converting all the names to lower case.
* If none of the baby's names contain the written number, there you can leave the number out of the table.
* Use `str_extract()`, not `str_extract_all()`. (We will ignore names where more than one of the words exists.)

*Hint*: You will have two steps that require pattern matching:
    1. Subset your table to only include the rows with the desired words.
    2. Add a column that contains the desired word.  


g. Which written number or numbers don't show up in any of the baby names?




h. Create a table that contains the names and their frequencies for the two least common written numbers.


i. List out the names that contain no vowels (consider "y" to be a vowel).  


### Problem 2: Tidying the "Call of the Wild"

Did you read "Call of the Wild" by Jack London?  If not, [read the first paragraph of its wiki page](https://en.wikipedia.org/wiki/The_Call_of_the_Wild) for a quick summary and then let's do some text analysis on this classic!  The following code will pull the book into R using the `gutenbergr` package.  

```{r}
data("stop_words")
library(gutenbergr)
wild <- gutenberg_download(215)
```

a.  Create a tidy text dataset where you tokenize by words.
```{r}

tidy_wild <- wild %>%
  unnest_tokens(output = word, input = text) 


```


b. Find the frequency of the 20 most common words.  First, remove stop words.
```{r}

tidy_wild_nostop <- tidy_wild %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  top_n(20) 

tidy_wild_nostop

```

c. Create a bar graph and a word cloud of the frequencies of the 20 most common words.
```{r}



# Barplot
tidy_wild_nostop %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)


# Remove punctuation, numeric characters, and other non-alphabetic characters
tidy_wild_cleaned <- tidy_wild_nostop %>%
  filter(!str_detect(word, "[^A-Za-z]"))  # Keep only words containing alphabetical characters



# Wordcloud
pal <- brewer.pal(9, "Set1")

wordcloud(words = tidy_wild_cleaned$word, freq = tidy_wild_cleaned$n, 
          scale = c(4, 1),
          rot.per = .5,
          colors = pal,
          random.order = FALSE
)

```

d. Explore the sentiment of the text using three of the sentiment lexicons in `tidytext`. What does your analysis say about the sentiment of the text?

```{r}

tidy_wild %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  count(sentiment, word, sort = TRUE) %>%
  group_by(sentiment) %>%
  slice_head(n = 20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free")

#seems to be more negative than positive

```

Notes:

* Make sure to NOT remove stop words this time.  
* `afinn` is a numeric score and should be handled differently than the categorical scores.



e. If you didn't do so in 2.d, compute the average sentiment score of the text using `afinn`.  Which positive words had the biggest impact? Which negative words had the biggest impact?

```{r}

#data("afinn")

# Compute sentiment scores for each word 
tidy_wild_sentiment <- tidy_wild %>%
  inner_join(afinn, by = "word")%>%
  group_by(word) %>%
  summarize(avg_sentiment = mean(value, na.rm = T))

# Calculate the average sentiment score
wild_sentiment <- mean(tidy_wild_sentiment$avg_sentiment, na.rm = T)

wild_sentiment
#on average: slightly negative



# looking for the most impact words:

# Find the top ten most negative words
top_negative <- tidy_wild_sentiment %>%
  filter(avg_sentiment < 0) %>%
  arrange(avg_sentiment) %>%
  head(10)

# Find the top ten most positive words
top_positive <- tidy_wild_sentiment %>%
  filter(avg_sentiment > 0) %>%
  arrange(desc(avg_sentiment)) %>%
  head(10)

# Print the top ten most negative and positive words
print(top_negative)
print(top_positive)

```


f. You should have found that "no" was an important negative word in the sentiment score.  To know if that really makes sense, let's turn to the raw lines of text for context.  Pull out all of the lines that have the word "no" in them.  Make sure to not pull out extraneous lines (e.g., a line with the word "now").  

```{r}
library(stringr)

# Split the text into sentences
wild_sentences <- str_split(tidy_wild_nostop$text, "(?<=[.!?])\\s+", simplify = TRUE)

# Find sentences containing the word "no"
wild_no_sentences <- wild_sentences[str_detect(wild_sentences, "\\bno\\b"), ]

# Print sentences containing the word "no"
print(wild_no_sentences)


#ok, it is not finding any, I don't know whats up
```

g. Draw some conclusions about how "no" is used in the text.
I can't, for some reasons it seems to think there are no instances of "no"

h. We can also look at how the sentiment of the text changes as the text progresses. Below, I have added two columns to the original dataset. Now I want you to do the following wrangling:

* Tidy the data (but don't drop stop words).
* Add the word sentiments using `bing`.
* Count the frequency of sentiments by index.
* Reshape the data to be wide with the count of the negative sentiments in one column and the positive in another, along with a column for index.
   - I don't no what to do with this, I already have these two columns and I can easily make a third that combines them and just join in the sentiment data set I         made with the original, I am confused by this step
* Compute a sentiment column by subtracting the negative score from the positive.
    

```{r}
wild_time <- wild %>%
  mutate(line = row_number(), index = floor(line/90) + 1) 
```

```{r, eval = FALSE}
#Hint: fill = 0 will insert zero instead of NA
#pivot_wider(..., values_fill = 0)
```

```{r}
#tidying
tidy_wild_time <- wild_time %>%
  unnest_tokens(output = word, input = text) 

#adding sentiments
tidy_wild_time_sentiments <- tidy_wild_time %>%
  inner_join(get_sentiments("bing"), by = c("word" = "word")) %>%
  group_by(index) %>%
  summarize(
    n_positive = sum(sentiment == "positive"),
    n_negative = sum(sentiment == "negative")
  ) %>%
  mutate(
    net_sentiment = (n_positive - n_negative)
  )

# View the resulting dataset
print(tidy_wild_time_sentiments)

# Inner join tidy_wild_time and tidy_wild_time_sentiments
joined_wild_time_sentiments <- inner_join(tidy_wild_time, tidy_wild_time_sentiments, by = "index")
```


i. Create a plot of the sentiment scores as the text progresses.
```{r}

joined_wild_time_sentiments %>% 
  ggplot(
    aes(x = index, y = net_sentiment)
  ) + geom_point()

#its kinda just all over the place

```



j. The choice of 45 lines per chunk was pretty arbitrary.  Try modifying the index value a few times and recreating the plot in i.  Based on your plots, what can you conclude about the sentiment of the novel as it progresses?

  -It seems like the sentiment is kinda just all over the place not really varying with the novel's linear progresion


k. Let's look at the bigrams (2 consecutive words).  Tokenize the text by bigrams.  

```{r}

wild_bigrams <- wild %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  mutate(i = row_number()) %>%    # add index for later grouping
  unnest_tokens(word, bigram, drop = FALSE) %>%    # tokenize bigrams into words
  group_by(i) %>%    # group by bigram index
  filter(n() == 2) %>%    # drop bigram instances where only one word left
  summarise(bigram = unique(bigram), .groups = "drop")


```


l.  Produce a sorted table that counts the frequency of each bigram and notice that stop words are still an issue.
```{r}


# Count the frequency of each bigram
wild_bigram_freq <- wild_bigrams %>%
  count(bigram, sort = TRUE)

# View the sorted table
print(wild_bigram_freq)



```

              